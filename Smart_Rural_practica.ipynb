{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicas en empresa - Smart Rural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AX_I7dNmQ_82"
   },
   "source": [
    "## Cargamos librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8xNjpYJRC10",
    "outputId": "a7adb19b-42c2-4920-8d1a-073e2d358527"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import cv2\n",
    "import math\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una funcion que nos crea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_images_with_labels(image_folder, label_folder, num_augmented_images):\n",
    "    transform = A.Compose(\n",
    "        [A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.CLAHE(clip_limit=2, p=0.2),\n",
    "        A.Transpose()],\n",
    "        bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'])\n",
    "    )\n",
    "\n",
    "    def load_labels(path):\n",
    "        try:\n",
    "            return np.loadtxt(path).reshape(-1, 5)\n",
    "        except:\n",
    "            return np.array([]).reshape(-1, 5)\n",
    "\n",
    "    output_image_folder = os.path.join(image_folder, 'augmented')\n",
    "    output_label_folder = os.path.join(label_folder, 'augmented')\n",
    "    os.makedirs(output_image_folder, exist_ok=True)\n",
    "    os.makedirs(output_label_folder, exist_ok=True)\n",
    "\n",
    "    for i in range(num_augmented_images):\n",
    "        random_image_file = np.random.choice(os.listdir(image_folder))\n",
    "        random_image_path = os.path.join(image_folder, random_image_file)\n",
    "        image = cv2.imread(random_image_path)\n",
    "        if image is None: \n",
    "            print(f\"Image at {random_image_path} could not be read.\")\n",
    "            continue\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        corresponding_label_file = os.path.splitext(random_image_file)[0] + '.txt'\n",
    "        label_path = os.path.join(label_folder, corresponding_label_file)\n",
    "\n",
    "        labels = load_labels(label_path)\n",
    "        if labels.size == 0:\n",
    "            continue\n",
    "\n",
    "        class_labels = labels[:, 0]\n",
    "        boxes = labels[:, 1:]\n",
    "        \n",
    "        transformed = transform(image=image, bboxes=boxes, class_labels=class_labels)\n",
    "        transformed_image = transformed['image']\n",
    "        transformed_bboxes = transformed['bboxes']\n",
    "        transformed_class_labels = transformed['class_labels']\n",
    "\n",
    "        output_image_path = os.path.join(output_image_folder, f'augmented_{i}.jpg')\n",
    "        transformed_image = cv2.cvtColor(transformed_image, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(output_image_path, transformed_image)\n",
    "        \n",
    "        augmented_boxes = np.array(transformed_bboxes)\n",
    "        augmented_class_labels = np.array(transformed_class_labels)\n",
    "        if len(augmented_class_labels) != 0 and len(augmented_boxes) != 0:\n",
    "            output_label_path = os.path.join(output_label_folder, f'augmented_{i}.txt')\n",
    "            augmented_labels = np.column_stack((augmented_class_labels, augmented_boxes))\n",
    "            np.savetxt(output_label_path, augmented_labels, fmt='%d %.6f %.6f %.6f %.6f')\n",
    "\n",
    "    print(f\"Las imagenes y las etiquetas han sido guardadas en {output_image_folder} y {output_label_folder} respectivamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicamos donde queremos en que ruta estan las imagenes y los labels que queremos aumentar y cuantas imagenes queremos crear, en nuestro caso 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_images_with_labels('datasets/data/images/val/', 'datasets/data/labels/val/', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFJxfsvKQ03a"
   },
   "source": [
    "## Entrenamos el modelo de detección de uvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la ruta donde estaran las carpetas con nuestras imagenes y el archivo de configuración para YOLOv8, este archivo de cocnfiguración "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0VJ5rU7QgSa"
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = '/datasets/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por temas de memoria se usa la version nano del modelo preentrenado de YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKIZ3wGLTgG0",
    "outputId": "1fdeea99-83fa-44ec-8563-7d5c2777f347"
   },
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QiWsHT1wVFym",
    "outputId": "71eec0ab-ac88-4fec-8f13-61b4feadd6f2"
   },
   "outputs": [],
   "source": [
    "results = model.train(data=os.path.join(ROOT_DIR,\"config.yaml\"), imgsz=800, augment=False, epochs = 100,optimizer='Adam', plots=True, batch=32, lr0 = 0.0001, weight_decay=0.0005, iou = 0.45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA1WF3ScgSEA"
   },
   "source": [
    "## Entrenamos el modelo para la retección de QR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0rnT_PWbgTyk",
    "outputId": "155ffcf0-b096-4da6-c341-529f5b40d29f"
   },
   "outputs": [],
   "source": [
    "results = model.train(data=os.path.join(ROOT_DIR,\"config_qr.yaml\"), imgsz=800, augment=True, epochs = 200,optimizer='Adam', plots=True, batch=32, lr0 = 0.0001, iou = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZMcfE9XyvOt",
    "outputId": "0370be98-b5a6-43a6-f5ca-5b1271e63431"
   },
   "source": [
    "## Predecimos y calculamos áreas y tamaños"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos las funciones necesarias, la primera es para dividir la ruta y quedarnos con el nombre de la imagen la segunda nos calcula la dimension del QR y la tercerda dada la dimension del QR la dimension de las uvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nombre_imagen(path):\n",
    "    nombre_imagen = path.split(\"\\\\\")[-1]\n",
    "    return nombre_imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_dim_qr(c_qr):\n",
    "    if not c_qr:  # Comprueba si la lista está vacía\n",
    "        return 0\n",
    "    \n",
    "    if isinstance(c_qr[0], list):  # Si c_qr es una lista de listas\n",
    "        total_pixels_to_cm2_ratio = 0\n",
    "        total_listas = len(c_qr)\n",
    "\n",
    "        for lista in c_qr:\n",
    "            ratio = calcula_dim_qr(lista)  # Llamada recursiva para cada lista individual\n",
    "            total_pixels_to_cm2_ratio += ratio\n",
    "\n",
    "        pixels_to_cm2_ratio = total_pixels_to_cm2_ratio / total_listas\n",
    "    else:\n",
    "        # Coordenadas xyxy de la caja delimitadora del cuadrado\n",
    "        x1_square, y1_square, x2_square, y2_square = c_qr[0], c_qr[1], c_qr[2], c_qr[3]\n",
    "\n",
    "        # Calcular el ancho y la altura de las cajas delimitadoras\n",
    "        box_width_square = x2_square - x1_square\n",
    "        box_height_square = y2_square - y1_square\n",
    "\n",
    "        # Calcular las áreas de las cajas delimitadoras en píxeles\n",
    "        area_square_pixels = box_width_square * box_height_square\n",
    "\n",
    "        # Sabemos el area del qr que es 324 cm²\n",
    "        area_square_real = 324 \n",
    "\n",
    "        # Calcular la relación entre los píxeles y el tamaño real\n",
    "        pixels_to_cm2_ratio = area_square_real / area_square_pixels\n",
    "    return pixels_to_cm2_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_dim_grape(c_grape, pixels_to_cm2_ratio):\n",
    "    if not c_grape:  # Comprueba si la lista está vacía\n",
    "        print(\"La lista está vacía.\")\n",
    "        return [[0,0,0]]\n",
    "\n",
    "    if isinstance(c_grape[0], list):  # Si c_grape es una lista de listas\n",
    "        results = []  # Lista para almacenar los resultados\n",
    "        for lista in c_grape:\n",
    "            if not lista:  # Comprueba si la sublista está vacía\n",
    "                print(\"La sublista está vacía.\")\n",
    "                continue  # Salta esta sublista y pasa a la siguiente\n",
    "            result = calcula_dim_grape(lista, pixels_to_cm2_ratio)  # Llamada recursiva para cada lista individual\n",
    "            results.append(result)  # Almacena los resultados\n",
    "        return results  # Devuelve la lista de resultados\n",
    "    else:\n",
    "        # Coordenadas xyxy de la caja delimitadora del racimo\n",
    "        x1_obj, y1_obj, x2_obj, y2_obj = c_grape[0], c_grape[1], c_grape[2], c_grape[3]\n",
    "\n",
    "        # Calcular el ancho y la altura de las cajas delimitadoras\n",
    "        box_width_obj = x2_obj - x1_obj\n",
    "        box_height_obj = y2_obj - y1_obj\n",
    "\n",
    "        # Calcular las áreas de las cajas delimitadoras en píxeles\n",
    "        area_obj_pixels = box_width_obj * box_height_obj\n",
    "\n",
    "        # Calcular la relación entre los píxeles y el tamaño real (cm) para el cuadrado\n",
    "        pixels_to_cm_ratio = math.sqrt(pixels_to_cm2_ratio)\n",
    "    \n",
    "        # Calcular el tamaño real del racimo\n",
    "        area_obj_real = area_obj_pixels * pixels_to_cm2_ratio \n",
    "\n",
    "        # Calcular las dimensiones estimadas del racimo\n",
    "        width_obj_real = box_width_obj * pixels_to_cm_ratio\n",
    "        height_obj_real = box_height_obj * pixels_to_cm_ratio\n",
    "\n",
    "        # print(f'El tamaño estimado del racimo es de {round(area_obj_real,2)} cm².')\n",
    "        # print(f'Las dimensiones estimadas del racimo son {round(width_obj_real,2)} cm x {round(height_obj_real,2)} cm.')\n",
    "        # print(area_obj_real, width_obj_real, height_obj_real)\n",
    "        return area_obj_real, width_obj_real, height_obj_real\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los modelos que mejor pesos nos han dado en los entrenamientos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qr = YOLO(\"runs/detect/trainQR/train/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grapes = YOLO(\"runs/detect/P42/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecimos con el dataset de test las uvas y los QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1597909298344.jpg: 608x800 1 qr, 302.0ms\n",
      "image 2/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1597909980616.jpg: 800x608 2 qrs, 296.0ms\n",
      "image 3/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1597924845589.jpg: 608x800 1 qr, 316.6ms\n",
      "image 4/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1597994671291.jpg: 608x800 2 qrs, 298.0ms\n",
      "image 5/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1598001053941.jpg: 608x800 2 qrs, 295.0ms\n",
      "image 6/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1598001818948.jpg: 608x800 2 qrs, 276.7ms\n",
      "image 7/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1598341617511.jpg: 608x800 2 qrs, 283.0ms\n",
      "image 8/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1598342953878.jpg: 608x800 2 qrs, 277.0ms\n",
      "image 9/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1599030766103.jpg: 608x800 1 qr, 259.6ms\n",
      "image 10/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1599032188801.jpg: 608x800 1 qr, 274.8ms\n",
      "image 11/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1602141746308.jpg: 608x800 1 qr, 286.3ms\n",
      "image 12/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1606994183464.jpg: 608x800 1 qr, 283.0ms\n",
      "image 13/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1606994877322.jpg: 608x800 1 qr, 274.0ms\n",
      "image 14/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1606995956803.jpg: 608x800 1 qr, 297.3ms\n",
      "Speed: 4.8ms preprocess, 287.1ms inference, 0.8ms postprocess per image at shape (1, 3, 800, 800)\n",
      "Results saved to \u001b[1mruns\\detect\\predict12\u001b[0m\n",
      "15 labels saved to runs\\detect\\predict12\\labels\n"
     ]
    }
   ],
   "source": [
    "results_qr = model_qr.predict(source=\"datasets/data/images/test\", save=True, save_txt=True, iou=0.5, line_width=5, conf=0.15, show_labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1597909298344.jpg: 608x800 21 grapess, 1053.1ms\n",
      "image 2/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1597909980616.jpg: 800x608 24 grapess, 1029.1ms\n",
      "image 3/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1597924845589.jpg: 608x800 11 grapess, 1013.6ms\n",
      "image 4/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1597994671291.jpg: 608x800 10 grapess, 986.2ms\n",
      "image 5/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1598001053941.jpg: 608x800 26 grapess, 1023.7ms\n",
      "image 6/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1598001818948.jpg: 608x800 28 grapess, 1050.0ms\n",
      "image 7/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1598341617511.jpg: 608x800 14 grapess, 1020.0ms\n",
      "image 8/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1598342953878.jpg: 608x800 21 grapess, 1116.0ms\n",
      "image 9/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1599030766103.jpg: 608x800 31 grapess, 1119.3ms\n",
      "image 10/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1599032188801.jpg: 608x800 28 grapess, 1048.8ms\n",
      "image 11/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1602141746308.jpg: 608x800 27 grapess, 1042.6ms\n",
      "image 12/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1606994183464.jpg: 608x800 29 grapess, 1072.6ms\n",
      "image 13/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1606994877322.jpg: 608x800 20 grapess, 1007.8ms\n",
      "image 14/14 C:\\Users\\administrador\\Documents\\MIA\\Practicas\\datasets\\data\\images\\test\\1606995956803.jpg: 608x800 18 grapess, 1034.7ms\n",
      "Speed: 4.4ms preprocess, 1044.1ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "Results saved to \u001b[1mruns\\detect\\predict13\u001b[0m\n",
      "15 labels saved to runs\\detect\\predict13\\labels\n"
     ]
    }
   ],
   "source": [
    "results_grapes = model_grapes.predict(source=\"datasets/data/images/test\", save=True, save_txt=True, iou=0.1, line_width=5, conf=0.2, show_labels=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos los datos de los bounding boxes de los QR y de las Uvas y calculamos las dimensiones de las uvas dadas las de los QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1597909298344.jpg\n",
      "1597909980616.jpg\n",
      "1597924845589.jpg\n",
      "1597994671291.jpg\n",
      "1598001053941.jpg\n",
      "1598001818948.jpg\n",
      "1598341617511.jpg\n",
      "1598342953878.jpg\n",
      "1599030766103.jpg\n",
      "1599032188801.jpg\n",
      "1602141746308.jpg\n",
      "1606994183464.jpg\n",
      "1606994877322.jpg\n",
      "1606995956803.jpg\n"
     ]
    }
   ],
   "source": [
    "list_results = []\n",
    "for i, (result_qr,result_grape) in enumerate(zip(results_qr,results_grapes)):\n",
    "    name = nombre_imagen(result_qr.path)\n",
    "    list_results.append([name])\n",
    "    print(name)\n",
    "    r = result_qr.boxes.xyxy.tolist()\n",
    "    g = result_grape.boxes.xyxy.tolist()\n",
    "    d = calcula_dim_qr(r)\n",
    "    datos = calcula_dim_grape(g,calcula_dim_qr(r))\n",
    "    for data in datos:\n",
    "        area, width, height = data\n",
    "        # print(area,width,height)\n",
    "        list_results.append(list(data))\n",
    "# print(list_results)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procesamos los datos para darlos en forma de dataframe y descargarlos en excel, por cada imagen nos muestra las detecciones de uvas que ha hecho su area su ancho y su largo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    ID  Conteo        Area      Ancho      Largo\n",
      "0    1597909298344.jpg       1  411.192880  17.277664  23.799101\n",
      "1    1597909298344.jpg       2  309.115069  12.597283  24.538233\n",
      "2    1597909298344.jpg       3  227.771083  11.850027  19.221144\n",
      "3    1597909298344.jpg       4  442.067162  15.874196  27.848161\n",
      "4    1597909298344.jpg       5  247.942293  15.164959  16.349684\n",
      "..                 ...     ...         ...        ...        ...\n",
      "303  1606995956803.jpg      14  346.757007  14.150853  24.504319\n",
      "304  1606995956803.jpg      15   98.042591   9.474602  10.347938\n",
      "305  1606995956803.jpg      16   72.106373   6.462968  11.156851\n",
      "306  1606995956803.jpg      17   66.216752   7.425076   8.917990\n",
      "307  1606995956803.jpg      18   67.916367   7.874709   8.624619\n",
      "\n",
      "[308 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Preprocesar los datos\n",
    "data = []\n",
    "identificador = []\n",
    "conteo_por_id = {}\n",
    "for line in list_results:\n",
    "    if len(line) == 1:  # Es un ID\n",
    "        identificador = line\n",
    "        conteo_por_id[identificador[0]] = 0  # Inicializar el conteo para el ID\n",
    "    else:  # Es un conjunto de valores\n",
    "        conteo_por_id[identificador[0]] += 1  # Incrementar el conteo para el ID\n",
    "        row =  identificador + [conteo_por_id[identificador[0]]] + line  # Añadimos el ID, los valores y el conteo\n",
    "        data.append(row)\n",
    "\n",
    "# Crear el dataframe\n",
    "df = pd.DataFrame(data, columns=[\"ID\", \"Conteo\", \"Area\", \"Ancho\", \"Largo\"])\n",
    "print(df)\n",
    "df.to_csv('total.csv',sep=';', decimal=',', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
